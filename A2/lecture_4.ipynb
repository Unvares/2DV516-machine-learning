{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4, Part 2\n",
    "\n",
    "1. Start by normalizing the data and separating a validation set with 20\\% of the data randomly selected. The remaining 80\\% will be called the sub-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import ForwardSelection\n",
    "import ROCAnalysis\n",
    "import MachineLearningModel\n",
    "\n",
    "importlib.reload(ForwardSelection)\n",
    "importlib.reload(ROCAnalysis)\n",
    "importlib.reload(MachineLearningModel)\n",
    "\n",
    "from ForwardSelection import ForwardSelection\n",
    "from ROCAnalysis import ROCAnalysis\n",
    "from MachineLearningModel import LogisticRegression\n",
    "\n",
    "data = np.genfromtxt('./resources/datasets/heart_disease_cleveland.csv', delimiter=',', skip_header=1) \n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "model = LogisticRegression()\n",
    "X_normalized = model.normalize(X)\n",
    "\n",
    "seed = 14159\n",
    "np.random.seed(seed)\n",
    "indices = np.random.permutation(len(X_normalized))\n",
    "split_point = int(0.8 * len(X_normalized))\n",
    "\n",
    "X_subdataset = X_normalized[indices[:split_point]]\n",
    "y_subdataset = y[indices[:split_point]]\n",
    "X_validation = X_normalized[indices[split_point:]]\n",
    "y_validation = y[indices[split_point:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use your implementation of forward selection to estimate a reasonable classification model. You must use your implementation of Logistic Regression in this assignment. The decision to make a reasonable number of iterations and learning rate is up to you but must be justified. Optimize the model selection to produce the best f-score. You must use the sub-dataset in your forward selection process. Report the features selected by this process and discuss your results.\n",
    "\n",
    "During the forward selection process, the following features were selected based on their contribution to the F-score:\n",
    "\n",
    "Current features: []. Best feature to add: 2 with F-score: 0.795  \n",
    "Current features: [2]. Best feature to add: 6 with F-score: 0.797  \n",
    "Current features: [2, 6]. Best feature to add: 3 with F-score: 0.811  \n",
    "Current features: [2, 6, 3]. Best feature to add: 0 with F-score: 0.811  \n",
    "Current features: [2, 6, 3, 0]. Best feature to add: 1 with F-score: 0.820  \n",
    "Current features: [2, 6, 3, 0, 1]. Best feature to add: 10 with F-score: 0.827  \n",
    "Current features: [2, 6, 3, 0, 1, 10]. Best feature to add: 11 with F-score: 0.860  \n",
    "Current features: [2, 6, 3, 0, 1, 10, 11]. Best feature to add: 4 with F-score: 0.860  \n",
    "Current features: [2, 6, 3, 0, 1, 10, 11, 4]. Best feature to add: 5 with F-score: 0.860  \n",
    "Current features: [2, 6, 3, 0, 1, 10, 11, 4, 5]. Best feature to add: 12 with F-score: 0.860  \n",
    "Current features: [2, 6, 3, 0, 1, 10, 11, 4, 5, 12]. Best feature to add: 7 with F-score: 0.853  \n",
    "Current features: [2, 6, 3, 0, 1, 10, 11, 4, 5, 12, 7]. Best feature to add: 8 with F-score: 0.785  \n",
    "Current features: [2, 6, 3, 0, 1, 10, 11, 4, 5, 12, 7, 8]. Best feature to add: 9 with F-score: 0.785  \n",
    "\n",
    "The highest F-score of 0.860 was achieve by the following set of features: [2, 6, 3, 0, 1, 10, 11]. It is important to note that using the entire dataset is not always the most optimal approach, as evidenced by the existence of techniques like Lasso regularization, so this result is completely normal. However, it is also important to note that forward selection is a greedy algorithm, so there might be a more optimal set of features. Luckily, for this task we can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
    "\n",
    "forward_selection = ForwardSelection(X_subdataset, y_subdataset, model, seed)\n",
    "forward_selection.forward_selection()\n",
    "selected_features = forward_selection.selected_features\n",
    "best_fscore = forward_selection.best_cost\n",
    "\n",
    "print(\"\\nSelected features:\", selected_features)\n",
    "print(f\"Best F-score: {best_fscore:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Report the performance of the best model in the validation set regarding all statistics available in your ROCAnalysis class. \n",
    "Was the process successful when compared to using all features?  \n",
    "Discuss your results regarding these metrics and what you can conclude from this experiment.\n",
    "\n",
    "Discussion:  \n",
    "The forward selection process selected a subset of features that resulted in a model with a precision of 0.719, recall of 0.731, and an F1 score of 0.725. These metrics indicate that the model has a good performance in identifying true positives with a balanced trade-off between precision and recall.\n",
    "\n",
    "However, when compared to the model using all features, the selected subset of features resulted in lower performance metrics. The model using all features achieved a precision of 0.809, recall of 0.846, and an F1 score of 0.827. This suggests that the additional features not selected by the forward selection process contribute valuable information that improves the model's ability to correctly classify the target variable.\n",
    "\n",
    "In conclusion, while the forward selection process was able to identify a subset of features that produced a well-performing model, the use of all features resulted in a better-performing model. This indicates that, in this case, the additional features provide important information that enhances the model's performance. Therefore, it may be beneficial to consider using all available features or exploring other feature selection methods to achieve the best possible model performance. However, it is also important to note that such a result is totally normal due to forward selection being a greedy algorithm. Also, in some cases, slight reduction in model performance might be preferrable if it provides a significant reduction in number of features as it will lead to a significant drop in computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_selected = X_validation[:, selected_features]\n",
    "\n",
    "y_pred = model.predict(X_validation_selected)\n",
    "y_validation_normalized = (y_validation >= 0.5).astype(int)\n",
    "y_pred_normalized = (y_pred >= 0.5).astype(int)\n",
    "roc_analysis = ROCAnalysis(y_validation_normalized, y_pred_normalized)\n",
    "\n",
    "print(\"Performance using forward selection:\")\n",
    "print(\"Precision:\", f\"{roc_analysis.precision():.3f}\")\n",
    "print(\"Recall (TP Rate):\", f\"{roc_analysis.tp_rate():.3f}\")\n",
    "print(\"False Positive Rate:\", f\"{roc_analysis.fp_rate():.3f}\")\n",
    "print(\"F1 Score:\", f\"{roc_analysis.f_score():.3f}\")\n",
    "\n",
    "model_all = LogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
    "model_all.fit(X_normalized, y)\n",
    "y_pred = model_all.predict(X_validation)\n",
    "y_pred = (y_pred >= 0.5).astype(int)\n",
    "roc_analysis_all_features = ROCAnalysis(y_validation, y_pred)\n",
    "\n",
    "print(\"\\nPerformance using all features:\")\n",
    "print(\"Precision:\", f\"{roc_analysis_all_features.precision():.3f}\")\n",
    "print(\"Recall (TP Rate):\", f\"{roc_analysis_all_features.tp_rate():.3f}\")\n",
    "print(\"False Positive Rate:\", f\"{roc_analysis_all_features.fp_rate():.3f}\")\n",
    "print(\"F1 Score:\", f\"{roc_analysis_all_features.f_score():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2DV516",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
