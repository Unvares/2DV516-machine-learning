{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 – Lecture 2\n",
    "\n",
    "## Lecture 2, Part 3\n",
    "\n",
    "1. Start by creating a procedure to split the dataset into training and test sets. The proportion must be 80% for training and 20% for testing. Show your procedure working by plotting a figure with 3 subplots. The first plot must be the dataset with all data. The second must be the training set and the third the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = np.loadtxt(\"./resources/datasets/secret_polynomial.csv\", delimiter=',', skiprows=1)\n",
    "X, y = data[:, 0], data[:, 1]\n",
    "\n",
    "# Split the dataset into training and test sets (80% training, 20% testing)\n",
    "np.random.seed(42)  # Ensure reproducibility\n",
    "indices = np.random.permutation(len(X))\n",
    "test_size = int(0.2 * len(X))\n",
    "\n",
    "train_indices = indices[test_size:]\n",
    "test_indices = indices[:test_size]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Plotting the dataset\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Define the limits for the plots based on the full dataset\n",
    "x_limits = (min(X), max(X))\n",
    "y_limits = (min(y), max(y))\n",
    "\n",
    "# Plot all data\n",
    "axs[0].scatter(X, y, color='blue', label='All Data')\n",
    "axs[0].set_title('All Data')\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_ylabel('y')\n",
    "axs[0].legend()\n",
    "axs[0].set_xlim(x_limits)\n",
    "axs[0].set_ylim(y_limits)\n",
    "\n",
    "# Plot training data\n",
    "axs[1].scatter(X_train, y_train, color='green', label='Training Set')\n",
    "axs[1].set_title('Training Set')\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[1].legend()\n",
    "axs[1].set_xlim(x_limits)\n",
    "axs[1].set_ylim(y_limits)\n",
    "\n",
    "# Plot test data\n",
    "axs[2].scatter(X_test, y_test, color='red', label='Test Set')\n",
    "axs[2].set_title('Test Set')\n",
    "axs[2].set_xlabel('X')\n",
    "axs[2].set_ylabel('y')\n",
    "axs[2].legend()\n",
    "axs[2].set_xlim(x_limits)\n",
    "axs[2].set_ylim(y_limits)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now fit and plot (e.g., using subplots) all polynomial models for degrees $d\\in [1,6]$. Observe your figure and decide which degree gives the best fit. Motivate your answer.\n",
    "\n",
    "When we randomly split the data into 80% training and 20% testing sets, using a fixed seed of 42, the best results are observed at polynomial degree 6. For this specific combination of training and testing data, polynomial degree 6 yields the lowest Mean Squared Error (MSE) and the highest coefficient of determination (R²) for the testing data.\n",
    "\n",
    "Here is the comparison table:\n",
    "\n",
    "Degree                           |                  MSE                  |            Relative Change (%)         |                  R^2                \n",
    "---------------------------------|---------------------------------------|----------------------------------------|--------------------------------------\n",
    "1                                | 12,952                                | -                                      | 0.276\n",
    "<span style=\"color:red\">2</span> | <span style=\"color:red\">13,235</span> | <span style=\"color:red\">+2.2%</span>   | <span style=\"color:red\">0.260</span>\n",
    "3                                | 4,434                                 | -66.5%                                 | 0.752\n",
    "4                                | 4,270                                 | -3.7%                                  | 0.761\n",
    "5                                | 4,238                                 | -0.7%                                  | 0.763\n",
    "<span style=\"color:lime\">6</span>| <span style=\"color:lime\">4,215</span> | <span style=\"color:lime\">-0.5%</span>  | <span style=\"color:lime\">0.764</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import MachineLearningModel\n",
    "\n",
    "importlib.reload(MachineLearningModel)\n",
    "from MachineLearningModel import RegressionModelNormalEquation\n",
    "\n",
    "def plot_polynomial_models(degrees, X_train, y_train, X_test, y_test, X):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20, 15))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, degree in enumerate(degrees):\n",
    "        model = RegressionModelNormalEquation(degree=degree)\n",
    "        X_train_normalized = model.normalize(X_train).reshape(-1, 1)\n",
    "        X_test_normalized = model.normalize(X_test).reshape(-1, 1)\n",
    "        \n",
    "        model.fit(X_train_normalized, y_train)\n",
    "        \n",
    "        axs[i].scatter(X_train_normalized, y_train, color='green', label='Training Set')\n",
    "        axs[i].scatter(X_test_normalized, y_test, color='red', label='Test Set')\n",
    "        \n",
    "        x_vals = np.linspace(min(model.normalize(X)), max(model.normalize(X)), 100).reshape(-1, 1)\n",
    "        y_vals = model.predict(x_vals)\n",
    "        \n",
    "        axs[i].plot(x_vals, y_vals, color='blue', linewidth=2, label=f'Degree {degree}')\n",
    "        axs[i].set_title(f'Polynomial Degree {degree}')\n",
    "        axs[i].set_xlabel('X')\n",
    "        axs[i].set_ylabel('y')\n",
    "        axs[i].legend()\n",
    "\n",
    "        r2 = model.score(X_test_normalized, y_test)\n",
    "        mse_train = model.evaluate(X_train_normalized, y_train)\n",
    "        mse_test = model.evaluate(X_test_normalized, y_test)\n",
    "        axs[i].text(0.95, 0.05, f'MSE (train): {int(mse_train)}\\nMSE (test): {int(mse_test)}\\nR$^2$: {r2:.3f}', \n",
    "                    ha='right', va='bottom', transform=axs[i].transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "        \n",
    "        # Print mse test and R^2 into the console for each polynomial degree\n",
    "        print(f'Degree {degree}: MSE (test) = {int(mse_test)}, R^2 = {r2:.3f}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "degrees = range(1, 7)\n",
    "plot_polynomial_models(degrees, X_train, y_train, X_test, y_test, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To increase the confidence of your answer, you must divide the data into training and test sets and make repeated runs with shuffled data (at least 20 runs). You must decide on the best way to make this decision. By using this approach, what is your decision and why?\n",
    "\n",
    "First, we need to address two main challenges when defining the best polynomial model:\n",
    "\n",
    "1. **Outliers**: Given the relatively small dataset, the 80% of training points we select might all be on one side of the graph, creating a dead zone for training.\n",
    "2. **Variance**: Due to the small dataset size, results can vary significantly between different runs, leading to high variance in model performance.\n",
    "\n",
    "To illustrate these issues, below I will show examples of the 3 worst and the 3 best shuffles, using seeds from 1 to 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MachineLearningModel)\n",
    "from MachineLearningModel import RegressionModelNormalEquation\n",
    "\n",
    "def evaluate_model(degree, X, y, test_size, seed):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    model = RegressionModelNormalEquation(degree=degree)\n",
    "    X_train_normalized = model.normalize(X_train)\n",
    "    X_test_normalized = model.normalize(X_test)\n",
    "\n",
    "    model.fit(X_train_normalized, y_train)\n",
    "\n",
    "    x_vals = np.linspace(min(model.normalize(X)), max(model.normalize(X)), 100)\n",
    "    y_vals = model.predict(x_vals)\n",
    "\n",
    "    r2 = model.score(X_test_normalized, y_test)\n",
    "    return r2, x_vals, y_vals, X_test_normalized, y_test\n",
    "\n",
    "def single_evaluation(degree, X, y, test_size=0.2):\n",
    "    test_size = int(X.shape[0] * test_size)\n",
    "    seeds = range(1, 201)\n",
    "    seed_r2_pairs = [(seed, evaluate_model(degree, X, y, test_size, seed)[0]) for seed in seeds]\n",
    "\n",
    "    lowest_r2_seeds = sorted(seed_r2_pairs, key=lambda x: x[1])[:3]\n",
    "    highest_r2_seeds = sorted(seed_r2_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "    for i, (seed, r2) in enumerate(lowest_r2_seeds + highest_r2_seeds):\n",
    "        r2, x_vals, y_vals, X_test_normalized, y_test = evaluate_model(degree, X, y, test_size, seed)\n",
    "        row = 0 if i < 3 else 1\n",
    "        col = i % 3\n",
    "        color = 'red' if row == 0 else 'green'\n",
    "        axs[row, col].plot(x_vals, y_vals, color=color, alpha=0.3, label=f'$R^2$: {r2:.3f}')\n",
    "        axs[row, col].scatter(X_test_normalized, y_test, color='blue', label='Test Data Points')\n",
    "        axs[row, col].set_title(f'Polynomial Degree {degree} (Seed: {seed})')\n",
    "        axs[row, col].set_xlabel('X')\n",
    "        axs[row, col].set_ylabel('y')\n",
    "        axs[row, col].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "single_evaluation(4, X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the shuffle, the R² value can drop as low as -4.\n",
    "\n",
    "Run the cell below to generate a boxplot from 200 different runs, including outliers. The result is quite horrible due to the anomalies caused by a few dozen unlucky shuffles.\n",
    "\n",
    "Interesting observation! If you run the cell multiple times, you might notice that polynomial degrees 4, 5, and 3 are the most affected by this issue, while degrees 1, 2, and 6 are the least affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_evaluation(degrees, X, y, n_runs=20, test_size=0.2, showfliers=True, print_means=False):\n",
    "    results = {degree: {'r2_scores': [], 'mse_test': []} for degree in degrees}\n",
    "    n_samples = X.shape[0]\n",
    "    test_size = int(n_samples * test_size)\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        np.random.seed(None) \n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        test_indices = indices[:test_size]\n",
    "        train_indices = indices[test_size:]\n",
    "        \n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        for degree in degrees:\n",
    "            model = RegressionModelNormalEquation(degree=degree)\n",
    "            X_train_normalized = model.normalize(X_train)\n",
    "            X_test_normalized = model.normalize(X_test).reshape(-1, 1)\n",
    "            \n",
    "            model.fit(X_train_normalized, y_train)\n",
    "            \n",
    "            r2 = model.score(X_test_normalized, y_test)\n",
    "            mse_test = model.evaluate(X_test_normalized, y_test)\n",
    "            \n",
    "            results[degree]['r2_scores'].append(r2)\n",
    "            results[degree]['mse_test'].append(mse_test)\n",
    "\n",
    "    if print_means:\n",
    "        mean_r2_scores = {degree: round(np.mean(results[degree]['r2_scores']), 3) for degree in degrees}\n",
    "        print(\"Mean R^2 values for each polynomial degree:\")\n",
    "        print(f\"{'Degree':<10}{'Mean R^2':<10}\")\n",
    "        print(\"-\" * 20)\n",
    "        for degree, mean_r2 in mean_r2_scores.items():\n",
    "            print(f\"{degree:<10}{mean_r2:<10}\")\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    for i, metric in enumerate(['r2_scores', 'mse_test']):\n",
    "        data = [results[degree][metric] for degree in degrees]\n",
    "        axs[i].boxplot(data, labels=[f'{d}' for d in degrees], showfliers=showfliers)\n",
    "        axs[i].set_title(metric.replace('_', ' ').title())\n",
    "        axs[i].set_xlabel('Polynomial Degree')\n",
    "        axs[i].set_ylabel(metric.replace('_', ' ').title())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "degrees = range(1, 7)\n",
    "repeated_evaluation(degrees, X, y, n_runs=200, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove the outliers and rerun the function to observe the true impact of polynomial degree on the coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 7)\n",
    "repeated_evaluation(degrees, X, y, n_runs=200, test_size=0.2, showfliers=False, print_means=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart clearly shows that polynomials of degree 1 and 2 do not efficiently regress the dataset, resulting in relatively low R^2 scores. In contrast, polynomials of degree 3 to 6 exhibit positive skewness. Based on our previous findings about variance and outliers, this skewness is likely due to the unlucky shuffles rather than the model itself. \n",
    "\n",
    "When selecting the polynomial degree that best addresses these dataset issues while providing precise results, degree 6 is still a reliable choice. Although polynomial degrees 3, 4 and 5 produce comparable outcomes, degree 6 generally offers a slightly higher mean and less extreme minimum. This can be easily verified by running the previous cell multiple times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2DV516",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
