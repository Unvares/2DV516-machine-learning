{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 – Lecture 2\n",
    "\n",
    "## Lecture 2 - Part 2\n",
    "\n",
    "1. Read the dataset and store the values in the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"./resources/datasets/housing-boston.csv\", 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "data = np.array(data[1:], dtype=float)\n",
    "X = data[:, [0, 1]]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the dataset. You must plot two figures side by side (e g., use the subplot method), with the predicted value as the $y-axis$ and each variable on the $x-axis$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, axis in enumerate(axs):\n",
    "    axs[i].scatter(X[:, i], y, marker=\".\", s=34.0, c='g', edgecolors='k', linewidths=0.2)\n",
    "    axs[i].set_xlabel('INDUS' if i == 0 else 'RM')\n",
    "    axs[i].set_ylabel('PRICE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use your implementation of the regression model with the normal equation (RegressionModelNormalEquation) and report:\n",
    "\n",
    "* The values for $\\beta$. \n",
    "\n",
    "* The cost.\n",
    "\n",
    "* The predicted value for an instance with values for INDUS and TAX equals to 2.31 and 6.575, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import MachineLearningModel\n",
    "\n",
    "importlib.reload(MachineLearningModel)\n",
    "\n",
    "from MachineLearningModel import RegressionModelNormalEquation\n",
    "\n",
    "model = RegressionModelNormalEquation(degree=1)\n",
    "\n",
    "model.fit(X, y)\n",
    "cost = model.evaluate(X, y)\n",
    "input_values = np.array([[2.31, 6.575]])\n",
    "prediction = model.predict(input_values)\n",
    "\n",
    "print(\n",
    "    f\"β1 = {round(model.theta[0], 2)}\\n\"\n",
    "    f\"β2 = {round(model.theta[1], 2)}\\n\"\n",
    "    f\"β3 = {round(model.theta[2], 2)}\\n\"\n",
    "    f\"MSE = {round(cost, 2)}\\n\"\n",
    "    f\"Prediction for INDUS = 2.31 and RM = 6.575 is {round(prediction[0], 2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now, normalize the input features, run the regression model with the normal equation, and report the same items. \n",
    "The predicted values for this experiment should be the same, but the $\\beta$ values change. Why?\n",
    "\n",
    "Normalization alters the value space of the x-axis for both input features, while leaving the y-axis value space unchanged. As a result, there is a need for adjustment of $\\beta$ values to make up for the change in the input values.\n",
    "\n",
    "Let's break down the impact of normalization on each feature:\n",
    "\n",
    "1. **INDUS:** The original INDUS values are distributed quite evenly, resulting in a smaller data variance. These values are also significantly larger than those representing standard deviation. As a result, the graph of the normalized dataset is narrower and centered further to the left on the x-axis compared to the original dataset. This requires a decrease in $\\beta2$ to meet the requirement for a more steep slope, and a slight decrease in $\\beta1$ to vertically align the line with the dataset.\n",
    "\n",
    "2. **RM:** The original RM values are densely placed with a few notiecable outliers, resulting in a larger data variance than in the INDUS case. The RM values are also quite close to those representing standard deviation. As a result, the graph of the normalized dataset is wider and centered at a much further to the left on the x-axis compared to the original dataset. This requires a slight decrease in $\\beta2$ to meet the requirement for a less steep slope, and a significant increase in $\\beta1$ to vertically align the line with the dataset.\n",
    "\n",
    "**Note:** The two plots below visually prove the statements above by demonstrating the differences between normalized and non-normalized datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MachineLearningModel)\n",
    "from MachineLearningModel import RegressionModelNormalEquation\n",
    "\n",
    "def predict(model, input_values, normalize=True):\n",
    "    if normalize:\n",
    "        input_values = (input_values - model.mean) / model.std\n",
    "    prediction = model.predict(input_values)\n",
    "    return prediction\n",
    "\n",
    "model = RegressionModelNormalEquation(degree=1)\n",
    "\n",
    "X_normalized = model.normalize(X)\n",
    "model.fit(X_normalized, y)\n",
    "cost = model.evaluate(X_normalized, y)\n",
    "\n",
    "input_values = np.array([[2.31, 6.575]])\n",
    "prediction = predict(model, input_values, normalize=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "for i, feature in enumerate(['INDUS', 'RM']):\n",
    "    axs[i].scatter(X[:, i], y, marker=\".\", s=34.0, c='gray', edgecolors='k', linewidths=0.2, alpha=0.2, label='Original Data')\n",
    "    axs[i].scatter(X_normalized[:, i], y, marker=\".\", s=34.0, c='g', edgecolors='k', linewidths=0.2, label='Normalized Data')\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel('PRICE')\n",
    "    \n",
    "    x_vals = np.linspace(min(X_normalized[:, i]), max(X_normalized[:, i]), 100)\n",
    "    y_vals = model.theta[0] + model.theta[i + 1] * x_vals\n",
    "    axs[i].plot(x_vals, y_vals, color='blue', linewidth=2, label='Regression Line')\n",
    "    axs[i].legend()\n",
    "\n",
    "textstr = (\n",
    "    f\"β1 = {round(model.theta[0], 2)}, β2 = {round(model.theta[1], 2)}, β3 = {round(model.theta[2], 2)}, MSE = {round(cost, 2)}\\n\"\n",
    "    f\"Prediction for INDUS = 2.31 and RM = 6.575 is {round(prediction[0], 2)}\"\n",
    ")\n",
    "\n",
    "plt.gcf().text(0.5, -0.1, textstr, fontsize=12, verticalalignment='bottom', horizontalalignment='center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now, you will work with your implementation of the gradient descent for any degree polynomial. In this part, you must compare how the cost function evolves by using your model using a non-normalized and a normalized instance of your RegressionModelGradientDescen class. \n",
    "    * You must plot two figures (e.g., use subplots) side by side to show how the cost evolves over 3000 iterations with a learning rate of $0.001$ using and not using feature normalization. \n",
    "    * Describe what is happening and why this happens (i.e., using or not normalization).        \n",
    "    \n",
    "\n",
    "### My answer\n",
    "\n",
    "Non-normalized input features exist on different scales, leading to several issues:\n",
    "\n",
    "1. **Incorrect Plots**: Input features share the same intercept. Without normalization, individual plots get a regression line with the right slope, but the wrong intercept.\n",
    "2. **Inconsistent Regression**: When features have large value ranges, more iterations are needed for the regression to converge because the gradients turn minuscule when approaching the minimum. Normalization reduces data variance, resulting in more effective gradients.\n",
    "3. **Numerical Instability & Precision Issues**: Larger feature magnitudes lead to larger gradients, requiring smaller learning rates for numerical stability and precision. Normalization ensures more consistent learning rates across different datasets.\n",
    "\n",
    "Using the Boston Housing dataset, I conducted two tests with different learning rates: 0.001 and 0.00001 (100 times smaller). Here are my findings:\n",
    "\n",
    "- **Learning Rate of 0.001**: The non-normalized dataset caused overflow issues during calculations, resulting in NaN values for all coefficients and the error metric.\n",
    "- **Learning Rate of 0.00001**: Not normalizing the dataset led to incorrect intercepts in individual plots and higher error rates. The non-normalized dataset quickly approached its final coefficient values within the first few iterations. However, it required approximately 80,000 iterations to converge, whereas the normalized dataset converged as early as the 10th iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MachineLearningModel)\n",
    "from MachineLearningModel import RegressionModelGradientDescent\n",
    "\n",
    "def train_and_evaluate_model(X, y, type=\"gd\", degree=1, learning_rate=0.001, num_iterations=3000, normalize=False):\n",
    "    # ne – Normal Equation\n",
    "    # gd – Gradient Descent\n",
    "    if type not in [\"ne\", \"gd\"]:\n",
    "        raise ValueError(\"type must be either 'ne' (normal equation) or 'gd' (gradient descent)\")\n",
    "    \n",
    "    if type == \"gd\":\n",
    "        model = RegressionModelGradientDescent(degree=degree, learning_rate=learning_rate, num_iterations=num_iterations)\n",
    "    elif type == \"ne\":\n",
    "        model = RegressionModelNormalEquation(degree=degree)\n",
    "    \n",
    "    if normalize:\n",
    "        X = model.normalize(X)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    cost = model.evaluate(X, y)\n",
    "\n",
    "    # Check for NaN or inf values in the model parameters or cost\n",
    "    if np.isnan(model.theta).any() or np.isinf(model.theta).any() or np.isnan(cost) or np.isinf(cost):\n",
    "        raise FloatingPointError(\"NaN or inf encountered in model parameters or cost\")\n",
    "\n",
    "    return model, cost, X\n",
    "\n",
    "def plot_cost_history(ax, cost_history, title, color, textstr):\n",
    "    ax.plot(range(len(cost_history)), cost_history, color=color, label=\"Cost History\")\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.text(0.5, 0.5, textstr, transform=ax.transAxes, fontsize=12, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"),\n",
    "            verticalalignment='bottom', horizontalalignment='left')\n",
    "\n",
    "def plot_feature_relationships(ax, X, y, model, feature_index, feature_name, scatter_color, line_color, label):\n",
    "    ax.scatter(X[:, feature_index], y, marker=\".\", s=34.0, c=scatter_color, edgecolors='k', linewidths=0.2, label=label)\n",
    "    x_vals = np.linspace(min(X[:, feature_index]), max(X[:, feature_index]), 100)\n",
    "    X_poly = model._polynomial_features(x_vals)\n",
    "    y_vals = X_poly @ model.theta[[0] + [j for j in range(feature_index + 1, len(model.theta), X.shape[1])]]\n",
    "    ax.plot(x_vals, y_vals, color=line_color, linewidth=2, label='Regression Line')\n",
    "    ax.set_xlabel(feature_name)\n",
    "    ax.set_ylabel('PRICE')\n",
    "    ax.legend()\n",
    "\n",
    "def generate_textstr(title, model, cost, input_values, prediction, learning_rate=None, iterations=None):\n",
    "    textstr = (\n",
    "        f\"{title}\\n\\n\"\n",
    "        f\"β1 = {round(model.theta[0], 2)}\\n\"\n",
    "        f\"β2 = {round(model.theta[1], 2)}\\n\"\n",
    "        f\"β3 = {round(model.theta[2], 2)}\\n\"\n",
    "        f\"MSE = {round(cost, 4)}\\n\\n\"\n",
    "        f\"Prediction:\\n {input_values[0]} –> {round(prediction[0], 2)}\"\n",
    "    )\n",
    "    if learning_rate is not None and iterations is not None:\n",
    "        textstr += (\n",
    "            f\"\\n\\nLearning Rate = {learning_rate}\\n\"\n",
    "            f\"Iterations = {iterations}\"\n",
    "        )\n",
    "    return textstr\n",
    "\n",
    "def main_execution():\n",
    "    try:\n",
    "        with np.errstate(over='raise', invalid='raise'):\n",
    "            # learning_rate=0.00001 is the minimum value non-normalized model doesn't overflow at with degree=1 (default)\n",
    "            model_non_normalized, cost_non_normalized, X_non_normalized = train_and_evaluate_model(X, y, normalize=False, type=\"gd\", learning_rate=0.00001)\n",
    "            model_normalized, cost_normalized, X_normalized = train_and_evaluate_model(X, y, normalize=True, type=\"gd\")\n",
    "\n",
    "            input_values = np.array([[2.31, 6.575]])\n",
    "            prediction_normalized = predict(model_normalized, input_values, normalize=True)\n",
    "            prediction_non_normalized = predict(model_non_normalized, input_values, normalize=False)\n",
    "\n",
    "            # Text for non-normalized model\n",
    "            textstr_non_normalized = generate_textstr(\n",
    "                \"Non-Normalized Model\", \n",
    "                model_non_normalized, \n",
    "                cost_non_normalized, \n",
    "                input_values, \n",
    "                prediction_non_normalized\n",
    "            )\n",
    "\n",
    "            # Text for normalized model\n",
    "            textstr_normalized = generate_textstr(\n",
    "                \"Normalized Model\", \n",
    "                model_normalized, \n",
    "                cost_normalized, \n",
    "                input_values, \n",
    "                prediction_normalized\n",
    "            )\n",
    "\n",
    "            # Plotting the cost function evolution\n",
    "            fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "            plot_cost_history(axs[0, 0], model_non_normalized.cost_history,  'Function Cost History (Non-Normalized)', 'red', textstr_non_normalized)\n",
    "            plot_cost_history(axs[0, 1], model_normalized.cost_history, 'Function Cost History (Normalized)', 'blue', textstr_normalized)\n",
    "\n",
    "            # Plotting relationships between individual features and labels with regression lines\n",
    "            features = ['INDUS', 'RM']\n",
    "            for i, feature in enumerate(features):\n",
    "                # Non-normalized data\n",
    "                plot_feature_relationships(axs[i + 1, 0], X_non_normalized, y, model_non_normalized, i, feature, 'g', 'red', 'Original Data')\n",
    "                axs[i + 1, 0].set_title(f'Relationship between {feature} and PRICE (Non-Normalized)')\n",
    "\n",
    "                # Normalized data\n",
    "                plot_feature_relationships(axs[i + 1, 1], X_normalized, y, model_normalized, i, feature, 'g', 'blue', 'Normalized Data')\n",
    "                axs[i + 1, 1].set_title(f'Relationship between {feature} and PRICE (Normalized)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except FloatingPointError as e:\n",
    "        print(f\"FloatingPointError encountered: {e}\")\n",
    "    except RuntimeWarning as e:\n",
    "        print(f\"RuntimeWarning encountered: {e}\")\n",
    "\n",
    "main_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Finally, find and plot a figure with the hyperparameter's learning rate and the number of iterations (using the normalized version) such that you get within a difference of 1\\% of the final cost for the normal equation using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the convergence rates for various combinations of learning rates and iteration numbers.\n",
    "The convergence rates will be categorized into four groups:\n",
    "1. Difference > 1%\n",
    "2. Difference < 1%\n",
    "3. Difference < 0.01%\n",
    "4. Difference < 0.000000001% (effectively zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_execution(learning_rate=0.001, num_iterations=10, degree=1):\n",
    "    try:\n",
    "        with np.errstate(over='raise', invalid='raise'):\n",
    "            model, cost, X_normalized = train_and_evaluate_model(X, y, type=\"gd\", normalize=True, learning_rate=learning_rate, num_iterations=num_iterations)\n",
    "            input_values = np.array([[2.31, 6.575]])\n",
    "            prediction = predict(model, input_values)\n",
    "            model_normal_eq, cost_normal_eq, _ = train_and_evaluate_model(X, y, type=\"ne\", normalize=True)\n",
    "            cost_difference = abs(cost - cost_normal_eq) / cost_normal_eq\n",
    "            return cost_difference\n",
    "    except FloatingPointError as e:\n",
    "        print(f\"FloatingPointError encountered: {e}\")\n",
    "    except RuntimeWarning as e:\n",
    "        print(f\"RuntimeWarning encountered: {e}\")\n",
    "\n",
    "def display_text(ax, textstr, textstr_normal_eq, textstr_difference):\n",
    "    ax.axis('off')\n",
    "    ax.text(0, 0.42, textstr, fontsize=12, verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    ax.text(0.8, 0.5, textstr_normal_eq, fontsize=12, verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    ax.text(0.8, 0.15, textstr_difference, fontsize=12, verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_iterations = 1\n",
    "results = []\n",
    "\n",
    "while True:\n",
    "    cost_difference = main_execution(learning_rate=learning_rate, num_iterations=num_iterations)\n",
    "    color = 'magenta' if cost_difference < 0.00000000001 else 'green' if cost_difference < 0.0001 else 'lightgreen' if cost_difference <= 0.01 else 'pink'\n",
    "    results.append((learning_rate, num_iterations, color))\n",
    "    \n",
    "    num_iterations += 1\n",
    "    if num_iterations > 50.0:\n",
    "        num_iterations = 1\n",
    "        learning_rate += 0.0001\n",
    "    if learning_rate >= 0.003:\n",
    "        break\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "learning_rates = sorted(set(lr for lr, _, _ in results))\n",
    "iterations = sorted(set(iters for _, iters, _ in results))\n",
    "\n",
    "for lr, iters, color in results:\n",
    "    ax.scatter(learning_rates.index(lr), iterations.index(iters), color=color)\n",
    "\n",
    "red_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='pink', markersize=10, label='Cost difference > 1%')\n",
    "lightgreen_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='Cost difference < 1%')\n",
    "green_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Cost difference < 0.01%')\n",
    "magenta_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='magenta', markersize=10, label='Cost difference < 0.000000001%')\n",
    "ax.legend(handles=[red_patch, lightgreen_patch, green_patch, magenta_patch])\n",
    "\n",
    "ax.set_xticks(range(len(learning_rates)))\n",
    "ax.set_xticklabels([f\"{lr:.4f}\" for lr in learning_rates], rotation=45)\n",
    "ax.set_yticks(range(len(iterations)))\n",
    "ax.set_yticklabels(iterations)\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Number of Iterations')\n",
    "ax.set_title('Convergence of Gradient Descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve a 1% difference in costs, a suitable combination is:\n",
    "- Number of iterations: 2\n",
    "- Learning rate: 0.0018\n",
    "\n",
    "For a more conservative approach, aiming for a cost difference as low as 0.01%, use:\n",
    "- Number of iterations: 5\n",
    "- Learning rate: between 0.0016 and 0.0019 (inclusive)\n",
    "\n",
    "For an extremely conservative approach, targeting an effectively 0 difference, use:\n",
    "- Number of iterations: 14\n",
    "- Learning rate: 0.0019\n",
    "\n",
    "Below, you can see visualizations of the cost history, resulting Mean Squared Error (MSE), coefficients, and predictions for the three combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_normal_eq, cost_normal_eq, _ = train_and_evaluate_model(X, y, type=\"ne\", normalize=True)\n",
    "cost_difference = abs(cost - cost_normal_eq) / cost_normal_eq\n",
    "\n",
    "learning_rates_arr = [0.0018, 0.0016, 0.0019]\n",
    "num_iterations_arr = [2, 5, 14]\n",
    "input_values = np.array([[2.31, 6.575]])\n",
    "\n",
    "for i in range(len(num_iterations_arr)):\n",
    "    model, cost, X_normalized = train_and_evaluate_model(X, y, type=\"gd\", normalize=True, learning_rate=learning_rates_arr[i], num_iterations=num_iterations_arr[i])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5), gridspec_kw={'width_ratios': [1, 4]})\n",
    "\n",
    "    textstr = generate_textstr(\n",
    "        \"Gradient Descent Model\",\n",
    "        model,\n",
    "        cost,\n",
    "        input_values,\n",
    "        predict(model, input_values),\n",
    "        learning_rate=learning_rates_arr[i],\n",
    "        iterations=num_iterations_arr[i]\n",
    "    )\n",
    "\n",
    "    textstr_normal_eq = generate_textstr(\n",
    "        \"Normal Equation Model\",\n",
    "        model_normal_eq,\n",
    "        cost_normal_eq,\n",
    "        input_values,\n",
    "        predict(model_normal_eq, input_values)\n",
    "    )\n",
    "\n",
    "    cost_difference = abs(cost - cost_normal_eq) / cost_normal_eq\n",
    "\n",
    "    textstr_difference = (\n",
    "        f\"Cost Difference:\\n\"\n",
    "        f\"{round(cost_difference * 100, 4)}%\"\n",
    "    )\n",
    "\n",
    "    ax[0].axis('off')\n",
    "    ax[0].text(0, 0.42, textstr, fontsize=12, verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    ax[0].text(0.8, 0.5, textstr_normal_eq, fontsize=12, verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "    ax[0].text(0.8, 0.15, textstr_difference, fontsize=12, verticalalignment='center', horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    ax[1].plot(range(1, len(model.cost_history) + 1), model.cost_history, 'blue')\n",
    "    ax[1].set_title('Cost History')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Cost')\n",
    "    ax[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
