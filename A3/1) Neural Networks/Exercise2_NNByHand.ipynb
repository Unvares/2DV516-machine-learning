{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d522bf",
   "metadata": {},
   "source": [
    "## Exercise 2, Neural networks 'by hand'\n",
    "The exercise is two-parted. First is to fill in the 'missing' code. The overall problem in the first part is to solve the XOR-problem using a neural network.\n",
    "\n",
    "In the second part you should copy your functional code into a new cell, and decrease the initailized weights by a factor of 1/10. What happens? Do you think that the problem is fixable in some way? How? Document your thoughts in a short text either in the notebook (preferred) or separately.\n",
    "\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f66bafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction before training ===\n",
      "Input: [0, 0], Target: 0, Prediction: 0.814\n",
      "Input: [0, 1], Target: 1, Prediction: 0.782\n",
      "Input: [1, 0], Target: 1, Prediction: 0.869\n",
      "Input: [1, 1], Target: 0, Prediction: 0.860\n",
      "Cross Entropy Loss: 1.010\n",
      "\n",
      "=== Start of Training ===\n",
      "Epoch 0, Loss: 1.010\n",
      "Epoch 10000, Loss: 0.071\n",
      "Epoch 20000, Loss: 0.028\n",
      "Epoch 30000, Loss: 0.017\n",
      "Epoch 40000, Loss: 0.012\n",
      "Epoch 50000, Loss: 0.010\n",
      "Epoch 60000, Loss: 0.008\n",
      "Epoch 70000, Loss: 0.007\n",
      "Epoch 80000, Loss: 0.006\n",
      "Epoch 90000, Loss: 0.005\n",
      "Epoch 100000, Loss: 0.005\n",
      "=== End of Training ===\n",
      "\n",
      "=== Prediction after training ===\n",
      "Input: [0, 0], Target: 0, Prediction: 0.004\n",
      "Input: [0, 1], Target: 1, Prediction: 0.995\n",
      "Input: [1, 0], Target: 1, Prediction: 0.996\n",
      "Input: [1, 1], Target: 0, Prediction: 0.004\n",
      "Cross Entropy Loss: 0.005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# sigmoid activation\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)  # Clip values to prevent overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "# cross-entropy loss function\n",
    "def cross_entropy_loss(out_nn, target):\n",
    "    epsilon = 1e-8\n",
    "    out_nn = np.clip(out_nn, epsilon, 1 - epsilon) # Clip values to avoid log(0)\n",
    "    return -np.mean(target * np.log(out_nn) + (1 - target) * np.log(1 - out_nn))\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(2, 2)\n",
    "b1 = np.random.randn(2, 1)\n",
    "W2 = np.random.randn(1, 2)\n",
    "b2 = np.random.randn(1, 1)\n",
    "\n",
    "# xor data\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "target = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "\n",
    "# forward\n",
    "def forward(X):\n",
    "    Z1 = W1 @ X + b1\n",
    "    Q1 = sigmoid(Z1)\n",
    "\n",
    "    Z2 = W2 @ Q1 + b2\n",
    "    out = sigmoid(Z2)\n",
    "\n",
    "    return out, Q1\n",
    "\n",
    "\n",
    "# backpropagation\n",
    "def backprop(X, target, out_nn, Q1):\n",
    "    delta2 = out_nn - target\n",
    "    delta1 = (W2.T @ delta2) * sigmoid_derivative(Q1)\n",
    "    dW2 = delta2 @ Q1.T\n",
    "    dW1 = delta1 @ X.T\n",
    "    db2 = np.mean(delta2, axis=1, keepdims=True)\n",
    "    db1 = np.mean(delta1, axis=1, keepdims=True)\n",
    "    return dW2, dW1, db2, db1\n",
    "\n",
    "\n",
    "# updating the weights\n",
    "def update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha):\n",
    "    W2 -= alpha * dW2\n",
    "    W1 -= alpha * dW1\n",
    "    b2 -= alpha * db2\n",
    "    b1 -= alpha * db1\n",
    "    return W2, W1, b2, b1\n",
    "\n",
    "\n",
    "def xor_nn_train_eval(X, target, W1, W2, b1, b2, alpha=0.1, epochs=100001):\n",
    "    # print the prediction of the XOR BEFORE training here\n",
    "    prediction = forward(X)\n",
    "    loss = cross_entropy_loss(prediction[0], target)\n",
    "\n",
    "    print(f\"=== Prediction before training ===\")\n",
    "    for i, value in enumerate(X.T):\n",
    "        print(f\"Input: [{value[0]}, {value[1]}], Target: {target[0][i]}, Prediction: {prediction[0][0][i]:.3f}\")\n",
    "    print(f\"Cross Entropy Loss: {loss:.3f}\\n\")\n",
    "\n",
    "    print(\"=== Start of Training ===\")\n",
    "    for epoch in range(epochs):\n",
    "        out, Q1 = forward(X)\n",
    "        loss = cross_entropy_loss(out, target)\n",
    "        dW2, dW1, db2, db1 = backprop(X, target, out, Q1)\n",
    "        W2, W1, b2, b1 = update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha)\n",
    "        if epoch % 10000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.3f}\")\n",
    "\n",
    "    print(\"=== End of Training ===\\n\")\n",
    "\n",
    "    # print the prediction of the XOR AFTER training here\n",
    "    prediction = forward(X)\n",
    "    loss = cross_entropy_loss(prediction[0], target)\n",
    "\n",
    "    print(f\"=== Prediction after training ===\")\n",
    "    for i, value in enumerate(X.T):\n",
    "        print(f\"Input: [{value[0]}, {value[1]}], Target: {target[0][i]}, Prediction: {prediction[0][0][i]:.3f}\")\n",
    "    print(f\"Cross Entropy Loss: {loss:.3f}\\n\")\n",
    "\n",
    "alpha = 0.1\n",
    "epochs = 100001\n",
    "xor_nn_train_eval(X, target, W1, W2, b1, b2, alpha, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a0e68",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "When the initial weights are reduced to one tenth, we can observe the following changes:\n",
    "1) Predictions before training have a lower cross entropy value\n",
    "2) Changes in weights, biases and, as a result, cross entropy value between epochs is marginal\n",
    "\n",
    "First, it's important to emphasize that the first change is unique to the XOR problem and wouldn't persist for other binary classification problems. Here's why: In the XOR problem, the binary nature of the inputs combined with smaller initial weights results in initial activations that are closer to zero. This leads to sigmoid outputs near 0.5, reducing the initial cross-entropy loss. However, in other binary classification problems, inputs might not be binary, and the network might require different transformations, meaning the same weight scaling wouldn't necessarily yield similar initial predictions or loss values.\n",
    "\n",
    "Second, the slower convergence rate comes from the fact that reducing initial weights causes very small pre-activation values in the hidden layer. These small pre-activations result in small gradients when backpropagated, even though the sigmoid function itself is in the high-gradient range around 0.5. Because the initial weights are tiny, each weight update remains small, causing minimal changes in the networkâ€™s behavior across epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61303af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction before training ===\n",
      "Input: [0, 0], Target: 0, Prediction: 0.522\n",
      "Input: [0, 1], Target: 1, Prediction: 0.522\n",
      "Input: [1, 0], Target: 1, Prediction: 0.524\n",
      "Input: [1, 1], Target: 0, Prediction: 0.524\n",
      "Cross Entropy Loss: 0.694\n",
      "\n",
      "=== Start of Training ===\n",
      "Epoch 0, Loss: 0.694\n",
      "Epoch 10000, Loss: 0.693\n",
      "Epoch 20000, Loss: 0.693\n",
      "Epoch 30000, Loss: 0.693\n",
      "Epoch 40000, Loss: 0.692\n",
      "Epoch 50000, Loss: 0.646\n",
      "Epoch 60000, Loss: 0.619\n",
      "Epoch 70000, Loss: 0.610\n",
      "Epoch 80000, Loss: 0.606\n",
      "Epoch 90000, Loss: 0.604\n",
      "Epoch 100000, Loss: 0.602\n",
      "=== End of Training ===\n",
      "\n",
      "=== Prediction after training ===\n",
      "Input: [0, 0], Target: 0, Prediction: 0.397\n",
      "Input: [0, 1], Target: 1, Prediction: 0.603\n",
      "Input: [1, 0], Target: 1, Prediction: 0.497\n",
      "Input: [1, 1], Target: 0, Prediction: 0.503\n",
      "Cross Entropy Loss: 0.602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "W1 = np.random.randn(2, 2) / 10\n",
    "b1 = np.random.randn(2, 1) / 10\n",
    "W2 = np.random.randn(1, 2) / 10\n",
    "b2 = np.random.randn(1, 1) / 10\n",
    "\n",
    "alpha = 0.1\n",
    "epochs = 100001\n",
    "xor_nn_train_eval(X, target, W1, W2, b1, b2, alpha, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
